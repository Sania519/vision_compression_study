{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import Levenshtein  # For CER calculation\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-OCR\"\n",
    "JSON_PATH = \"./focus_benchmark_test/en_page_ocr.json\"\n",
    "IMAGE_DIR = \"./focus_benchmark_test/en_pdf_png\"\n",
    "OUTPUT_PATH = \"./fox_results_table.json\"\n",
    "PROMPT = \"<image>\\nFree OCR.\"\n",
    "MAX_TOKENS = 8192\n",
    "PATCH_SIZE = 16  # Patch size for vision token estimation\n",
    "VISION_TOKENS_LIST = [64, 100, 1000]  # Vision tokens to evaluate\n",
    "\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    fox_data = json.load(f)\n",
    "print(f\"Loaded {len(fox_data)} samples from Fox benchmark\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    enable_prefix_caching=False,\n",
    "    mm_processor_cache_gb=0,\n",
    "    logits_processors=[NGramPerReqLogitsProcessor],\n",
    ")\n",
    "\n",
    "\n",
    "model_inputs = []\n",
    "for sample in fox_data:\n",
    "    img_file = sample[\"image\"]\n",
    "    img_path = os.path.join(IMAGE_DIR, img_file)\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {img_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    model_inputs.append({\n",
    "        \"prompt\": PROMPT,\n",
    "        \"multi_modal_data\": {\"image\": img},\n",
    "        \"sample_id\": img_file,\n",
    "        \"ground_truth\": sample[\"conversations\"][1][\"value\"]\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(model_inputs)} valid inputs\")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    extra_args=dict(\n",
    "        ngram_size=30,\n",
    "        window_size=90,\n",
    "        whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n",
    "    ),\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "print(\"Running inference...\")\n",
    "model_outputs = llm.generate(model_inputs, sampling_params)\n",
    "\n",
    "def cer_precision(pred_text, gt_text):\n",
    "    \"\"\"Compute CER-based precision (1 - CER)\"\"\"\n",
    "    cer = Levenshtein.distance(pred_text, gt_text) / max(len(gt_text), 1)\n",
    "    return 1 - cer\n",
    "\n",
    "def estimate_vision_tokens(img, patch_size=PATCH_SIZE):\n",
    "    \"\"\"Estimate number of visual tokens for the image\"\"\"\n",
    "    w, h = img.size\n",
    "    return (w // patch_size) * (h // patch_size)\n",
    "\n",
    "def text_token_range(num_tokens):\n",
    "    \"\"\"Return range string for table grouping\"\"\"\n",
    "    lower = (num_tokens // 100) * 100\n",
    "    upper = lower + 100\n",
    "    return f\"{lower}-{upper}\"\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for inp, out in zip(model_inputs, model_outputs):\n",
    "    pred_text = out.outputs[0].text\n",
    "    gt_text = inp[\"ground_truth\"]\n",
    "\n",
    "    precision = cer_precision(pred_text, gt_text)\n",
    "    num_text_tokens = len(tokenizer.encode(gt_text))\n",
    "\n",
    "    # Get image to estimate vision tokens\n",
    "    img_path = os.path.join(IMAGE_DIR, inp[\"sample_id\"])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    base_vision_tokens = estimate_vision_tokens(img)\n",
    "\n",
    "    for vt in VISION_TOKENS_LIST:\n",
    "        compression = num_text_tokens / vt  # compute \"×\" as in paper\n",
    "        table_data.append({\n",
    "            \"Text Tokens\": text_token_range(num_text_tokens),\n",
    "            \"Vision Tokens\": vt,\n",
    "            \"Precision (%)\": round(precision*100, 1),\n",
    "            \"Compression (×)\": round(compression, 1),\n",
    "            \"Pages\": 1\n",
    "        })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(table_data)\n",
    "df_grouped = df.groupby([\"Text Tokens\", \"Vision Tokens\"]).agg({\n",
    "    \"Precision (%)\": \"mean\",\n",
    "    \"Compression (×)\": \"mean\",\n",
    "    \"Pages\": \"sum\"\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH) or \".\", exist_ok=True)\n",
    "df_grouped.to_json(OUTPUT_PATH, orient=\"records\", indent=2)\n",
    "print(f\"Saved table to {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
